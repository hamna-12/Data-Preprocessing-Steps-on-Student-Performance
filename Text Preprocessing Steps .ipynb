{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cece6e-654d-47f5-bca7-991c5108dfd3",
   "metadata": {},
   "source": [
    "Text Preprocessing Steps\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b93563-1ca1-4898-bd96-8e335ba71320",
   "metadata": {},
   "source": [
    "Importing Libraries\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d2f0ac-a5ae-4a0d-91fb-85246c8052ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # main NLP library\n",
    "from nltk.tokenize import word_tokenize  # split text into words\n",
    "from nltk.corpus import stopwords        # common words like 'is', 'the'\n",
    "from nltk.stem import PorterStemmer      # reduce words to root form\n",
    "from nltk.stem import WordNetLemmatizer  # convert words to dictionary form\n",
    "from nltk import pos_tag                 # label words as noun, verb, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "689e9b74-3a16-41b0-b1c8-df178874df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')                     \n",
    "nltk.download('stopwords')                \n",
    "nltk.download('wordnet')                 \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ea9f5-de32-4392-8c18-b9f27c8931d2",
   "metadata": {},
   "source": [
    " INPUT TEXT\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f01a873-742e-4c39-80f2-31c8a8c2c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hamna is learning Python in the NLP class, and she finds it fun and challenging!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ccb00a-29f2-4b1e-a204-8c4e25af37c5",
   "metadata": {},
   "source": [
    "Step 1: Convert sentence to lower case\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b385ca8-c26d-41dc-a2f7-d9b0450931db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased: hamna is learning python in the nlp class, and she finds it fun and challenging!\n"
     ]
    }
   ],
   "source": [
    "sentence_lower = sentence.lower()   #Convert the whole sentence into lowercase letters \n",
    "print(\"Lowercased:\", sentence_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33357b-e4cc-4fab-90cd-a3d47f64c6ec",
   "metadata": {},
   "source": [
    "Step 2: Tokenization\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91663a0-4501-4659-b8c0-36b604d87c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Tokens: ['Hamna', 'is', 'learning', 'Python', 'in', 'the', 'NLP', 'class', ',', 'and', 'she', 'finds', 'it', 'fun', 'and', 'challenging', '!']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the sentence into words\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"\\n1. Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd7094-0f67-45a1-b0b6-fbf72fd0a74a",
   "metadata": {},
   "source": [
    "Step 3: Stopword Removal\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d0bec19-2ffc-4a9a-9245-ab8ab3dfac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. After Stopword Removal: ['Hamna', 'learning', 'Python', 'NLP', 'class', 'finds', 'fun', 'challenging']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords and non-alphabetic tokens (like punctuation and numbers)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))           # Get the list of English stopwords\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words and w.isalpha()]     # Remove stopwords and keep only alphabetic words\n",
    "print(\"\\n2. After Stopword Removal:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937cd72-3535-461d-b6e0-c22f6c0bab84",
   "metadata": {},
   "source": [
    "Step 4: Stemming\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8263ae79-0d2a-4f7c-89b3-7e0075706255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. After Stemming: ['hamna', 'learn', 'python', 'nlp', 'class', 'find', 'fun', 'challeng']\n"
     ]
    }
   ],
   "source": [
    "# Apply Stemming (reduces words to root form)\n",
    "\n",
    "ps = PorterStemmer()              # create stemmer\n",
    "stemmed = [ps.stem(w) for w in filtered]  # reduce words to root form\n",
    "print(\"\\n3. After Stemming:\", stemmed)   # show result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30e61c-cfdb-438b-b234-5abd088e20f1",
   "metadata": {},
   "source": [
    "Step 5: lemmatization\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe7592e-beaf-47af-82f8-478cc842c96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. After Lemmatization: ['Hamna', 'learning', 'Python', 'NLP', 'class', 'find', 'fun', 'challenging']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()              # create lemmatizer\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in filtered]  # convert words to dictionary form\n",
    "print(\"\\n4. After Lemmatization:\", lemmatized)            # show result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291091df-b67c-4109-a2d3-b446c8cb7b8b",
   "metadata": {},
   "source": [
    "Step 6: POS tagging\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9a66085-66f3-4399-9396-f248e67f635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. POS Tagging: [('Hamna', 'NNP'), ('is', 'VBZ'), ('learning', 'VBG'), ('Python', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('NLP', 'NNP'), ('class', 'NN'), (',', ','), ('and', 'CC'), ('she', 'PRP'), ('finds', 'VBZ'), ('it', 'PRP'), ('fun', 'NN'), ('and', 'CC'), ('challenging', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "#  Part-of-Speech (POS) Tagging\n",
    "\n",
    "pos_tags = pos_tag(tokens)          # Assigns POS tags to each token\n",
    "print(\"\\n5. POS Tagging:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985a808-ec13-4104-aba2-d12c5bc48d35",
   "metadata": {},
   "source": [
    "TASK 2\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4ad319e-2087-4ffd-9f55-acb065004450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer       #convert text data into a matrix of token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb93ee7-5128-4d41-a29c-52e460e4ae70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    " Step 1: Input corpus (list of sentences)\n",
    " -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38b9b31e-82c7-4eb1-a62e-b961f1755879",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I am loving the NLP class, but sometimes it feels confusing\"\n",
    "sentence2 = \"NLP is a fascinating field it deals with text speech and language understanding\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305eb1d-0076-408a-9767-4652d57e42d8",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5c9c219-580d-407e-9c04-0de4cf4af62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    # Lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove punctuation\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Split into words\n",
    "    words = sentence.split()\n",
    "    # Optional: remove stopwords\n",
    "    stopwords = ['the', 'is', 'it', 'a', 'am', 'but', 'with', 'and']\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    return words\n",
    "\n",
    "# Apply preprocessing\n",
    "word1 = preprocess(sentence1)\n",
    "word2 = preprocess(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a97a6-d958-454c-b49a-3089cff3652d",
   "metadata": {},
   "source": [
    "Step 2: Make all words lowercase and split into list of words\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fc395c8-7a49-44bd-b78e-9978ad2194ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'loving', 'the', 'nlp', 'class,', 'but', 'sometimes', 'it', 'feels', 'confusing']\n",
      "['nlp', 'is', 'a', 'fascinating', 'field', 'it', 'deals', 'with', 'text', 'speech', 'and', 'language', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "#  Make all words lowercase and split into list of words\n",
    "words1 = sentence1.lower().split()\n",
    "words2 = sentence2.lower().split()\n",
    "print(words1)\n",
    "print(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d7e74-7a47-4514-b728-22028fc0ac03",
   "metadata": {},
   "source": [
    "Step 3: Create vocabulary\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c692e694-ed9a-40bd-8a3d-68693311bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'speech', 'fascinating', 'but', 'confusing', 'with', 'field', 'sometimes', 'loving', 'language', 'text', 'deals', 'class,', 'am', 'understanding', 'is', 'nlp', 'a', 'i', 'the', 'it', 'feels']\n"
     ]
    }
   ],
   "source": [
    " vocabulary = list(set(words1 + words2))\n",
    "print(vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36ff42-6caf-4e7b-82cd-3790672c6081",
   "metadata": {},
   "source": [
    "Step 4:  Make Bag of Words (BOW) for each sentence\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9afe4756-d0b7-4166-a750-e698ee76eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW for sentence 1: [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "BOW for sentence 2: [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#  Make Bag of Words (BOW) for each sentence\n",
    "bow1 = [words1.count(word) for word in vocabulary]\n",
    "bow2 = [words2.count(word) for word in vocabulary]      # For each word in the vocabulary, count how many times it appears in words1 and word2\n",
    "\n",
    "print(\"BOW for sentence 1:\", bow1)\n",
    "print(\"BOW for sentence 2:\", bow2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d3be8-7d36-4f63-a017-26cac8bea7e9",
   "metadata": {},
   "source": [
    "Term Frequency (TF)\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b92171fc-76a8-4584-abe7-4cebb92bc047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF for sentence 1: [0.0, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.09090909090909091, 0.0, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091]\n",
      "TF for sentence 2: [0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.0, 0.0, 0.07692307692307693, 0.07692307692307693, 0.0, 0.0, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.0, 0.0, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.0, 0.0, 0.07692307692307693, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# For each word in the vocabulary:\n",
    "# - Count how many times the word appears in the sentence\n",
    "# - Divide by total number of words in that sentence\n",
    "# This gives a \"relative frequency\" (between 0 and 1)\n",
    "\n",
    "\n",
    "tf1 = [words1.count(word) / len(words1) for word in vocabulary]\n",
    "tf2 = [words2.count(word) / len(words2) for word in vocabulary]\n",
    "\n",
    "print(\"\\nTF for sentence 1:\", tf1)\n",
    "print(\"TF for sentence 2:\", tf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261bc934-56ad-41aa-ab52-0d3db5ec3632",
   "metadata": {},
   "source": [
    "IDF (Inverse Document Frequency)\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f608fc8-68b3-4c83-89a2-7de9f5ae4811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.0, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.0, 0.6931471805599453]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "documents = [words1, words2]\n",
    "idf = []\n",
    "\n",
    "for word in vocabulary:\n",
    "    doc_count = sum([1 for doc in documents if word in doc])\n",
    "    idf.append(math.log(len(documents)/doc_count))\n",
    "\n",
    "print(\"IDF:\", idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90603abf-be69-4e4b-aa7f-ee9295df1b0e",
   "metadata": {},
   "source": [
    "TF Ã— IDF\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fa725d7-02e1-411f-b24c-a6621a4be445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF for sentence 1: [0.0, 0.0, 0.0, 0.06301338005090412, 0.06301338005090412, 0.0, 0.0, 0.06301338005090412, 0.06301338005090412, 0.0, 0.0, 0.0, 0.06301338005090412, 0.06301338005090412, 0.0, 0.0, 0.0, 0.0, 0.06301338005090412, 0.06301338005090412, 0.0, 0.06301338005090412]\n",
      "TF-IDF for sentence 2: [0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.0, 0.0, 0.053319013889226566, 0.053319013889226566, 0.0, 0.0, 0.053319013889226566, 0.053319013889226566, 0.053319013889226566, 0.0, 0.0, 0.053319013889226566, 0.053319013889226566, 0.0, 0.053319013889226566, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "tfidf1 = [tf1[i] * idf[i] for i in range(len(vocabulary))]\n",
    "tfidf2 = [tf2[i] * idf[i] for i in range(len(vocabulary))]\n",
    "\n",
    "print(\"\\nTF-IDF for sentence 1:\", tfidf1)\n",
    "print(\"TF-IDF for sentence 2:\", tfidf2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418a88a-b480-4c6c-8f4d-71a5d7891847",
   "metadata": {},
   "source": [
    "WORD EMBEDDING\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e5c1bd2-65e4-4dc4-b836-96738671adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cfce769c-be1e-4e7e-8aaf-cda88f63a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['nlp', 'understanding', 'language', 'speech', 'text', 'deals', 'field', 'fascinating', 'confusing', 'feels', 'sometimes', 'class', 'loving', 'i']\n",
      "Vector for 'nlp': [-0.00107245  0.00047286  0.0102067   0.01801855 -0.0186059 ]\n",
      "Most similar to 'nlp': [('class', 0.16704076528549194), ('i', 0.15019884705543518), ('fascinating', 0.13204392790794373), ('language', 0.1267007291316986), ('feels', 0.0998455360531807), ('understanding', 0.042373016476631165), ('loving', 0.04067763686180115), ('deals', 0.012442179024219513), ('sometimes', -0.01259106956422329), ('speech', -0.01447527389973402)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = [word1, word2]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "print(\"Vocabulary:\", list(model.wv.index_to_key))\n",
    "\n",
    "print(\"Vector for 'nlp':\", model.wv['nlp'][:5]) \n",
    "\n",
    "print(\"Most similar to 'nlp':\", model.wv.most_similar('nlp'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
